{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.iforest import IsolationForest\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score\n",
    "from util.utils import find_TPR_threshold\n",
    "from sklearn.ensemble import IsolationForest as skIsolationForest\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Wednesday-workingHours.pcap_ISCX.csv\n",
      "Dataset shape: (692703, 78)\n",
      "Benign samples: 440031\n",
      "Anomalies in dataset: 252672\n",
      "Contamination rate: 36.4762 %\n",
      "\n",
      "Dataset: Monday-WorkingHours.pcap_ISCX.csv\n",
      "Dataset shape: (529919, 78)\n",
      "Benign samples: 529918\n",
      "Anomalies in dataset: 1\n",
      "Contamination rate: 0.0002 %\n",
      "\n",
      "Dataset: Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "Dataset shape: (445909, 78)\n",
      "Benign samples: 432074\n",
      "Anomalies in dataset: 13835\n",
      "Contamination rate: 3.1027 %\n",
      "\n",
      "Dataset: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "Dataset shape: (286467, 78)\n",
      "Benign samples: 127537\n",
      "Anomalies in dataset: 158930\n",
      "Contamination rate: 55.4793 %\n",
      "\n",
      "Dataset: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "Dataset shape: (225745, 78)\n",
      "Benign samples: 97718\n",
      "Anomalies in dataset: 128027\n",
      "Contamination rate: 56.7131 %\n",
      "\n",
      "Dataset: Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "Dataset shape: (191033, 78)\n",
      "Benign samples: 189067\n",
      "Anomalies in dataset: 1966\n",
      "Contamination rate: 1.0291 %\n",
      "\n",
      "Dataset: Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "Dataset shape: (288602, 78)\n",
      "Benign samples: 288566\n",
      "Anomalies in dataset: 36\n",
      "Contamination rate: 0.0125 %\n",
      "\n",
      "Dataset: Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "Dataset shape: (170366, 78)\n",
      "Benign samples: 168186\n",
      "Anomalies in dataset: 2180\n",
      "Contamination rate: 1.2796 %\n",
      "\n",
      "Complete dataset shape: (2830744, 78)\n",
      "Benign samples: 2273097\n",
      "Anomalies in dataset: 557647\n",
      "Contamination rate: 19.6997 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "filenames = os.listdir('../datasets/CIC-IDS-17')\n",
    "datasets = []\n",
    "for filename in filenames:\n",
    "    data = pd.read_csv(f'../datasets/CIC-IDS-17/{filename}')\n",
    "    X = data.drop(' Label', axis=1)\n",
    "    y = pd.Series([0 if x == 'BENIGN' else 1 for x in data[' Label']])\n",
    "    datasets.append((X, y, filename))\n",
    "\n",
    "    print(f'Dataset: {filename}')\n",
    "    print(f'Dataset shape: {X.shape}')\n",
    "    print(f'Benign samples: {y.value_counts()[0]}')\n",
    "    print(f'Anomalies in dataset: {y.value_counts().get(1, 0)}')\n",
    "    print(f'Contamination rate: {y.value_counts().get(1, 0) / y.count() * 100:.4f} %\\n')\n",
    "\n",
    "complete_dataset = (pd.concat([x[0] for x in datasets]), pd.concat([x[1] for x in datasets]))\n",
    "print(f'Complete dataset shape: {complete_dataset[0].shape}')\n",
    "print(f'Benign samples: {complete_dataset[1].value_counts()[0]}')\n",
    "print(f'Anomalies in dataset: {complete_dataset[1].value_counts().get(1, 0)}')\n",
    "print(f'Contamination rate: {complete_dataset[1].value_counts().get(1, 0) / complete_dataset[1].count() * 100:.4f} %\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Isolation Forest on Wednesday-workingHours.pcap_ISCX.csv\n",
      "sample_size=50000, n_trees=300, training time: 42.781s\n",
      "\n",
      "Training Isolation Forest on Monday-WorkingHours.pcap_ISCX.csv\n",
      "sample_size=50000, n_trees=300, training time: 36.648s\n",
      "\n",
      "Training Isolation Forest on Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "sample_size=50000, n_trees=300, training time: 37.896s\n",
      "\n",
      "Training Isolation Forest on Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "sample_size=50000, n_trees=300, training time: 40.963s\n",
      "\n",
      "Training Isolation Forest on Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "sample_size=50000, n_trees=300, training time: 46.443s\n",
      "\n",
      "Training Isolation Forest on Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "sample_size=50000, n_trees=300, training time: 43.058s\n",
      "\n",
      "Training Isolation Forest on Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "sample_size=50000, n_trees=300, training time: 42.382s\n",
      "\n",
      "Training Isolation Forest on Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "sample_size=50000, n_trees=300, training time: 43.045s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Isolation Forest\n",
    "clf_forests = []\n",
    "sample_size = 50000\n",
    "n_trees = 300\n",
    "for X, y, filename in datasets:\n",
    "    print(f'Training Isolation Forest on {filename}')\n",
    "    clf = IsolationForest(sample_size, n_trees)\n",
    "    start_time = time.time()\n",
    "    clf.fit(X, improved=False)\n",
    "    clf_forests.append((clf, filename))\n",
    "    end_time = time.time()\n",
    "    print(f'{sample_size=}, {n_trees=}, training time: {end_time - start_time:.3f}s\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on Wednesday-workingHours.pcap_ISCX.csv\n",
      "Dataset shape: (692703, 78)\n",
      "Benign samples: 440031\n",
      "Anomalies in dataset: 252672\n",
      "Contamination rate: 36.4762 %\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for (clf, filename), (X, y, _) in zip(clf_forests, datasets):\n",
    "    print(f'Predicting on {filename}')\n",
    "    print(f'Dataset shape: {X.shape}')\n",
    "    print(f'Benign samples: {y.value_counts()[0]}')\n",
    "    print(f'Anomalies in dataset: {y.value_counts().get(1, 0)}')\n",
    "    print(f'Contamination rate: {y.value_counts().get(1, 0) / y.count() * 100:.4f} %\\n')\n",
    "    \n",
    "    # Predict\n",
    "    start_time = time.time()\n",
    "    scores = clf.anomaly_score(X)\n",
    "    threshold, FPR = find_TPR_threshold(y, scores, 0.9)\n",
    "    y_pred = clf.predict_from_anomaly_scores(scores, threshold)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Metrics\n",
    "    plt.hist(scores, bins=30, label=f'{filename}')\n",
    "    print(f'Prediction time: {end_time - start_time:.3f}s')\n",
    "    print('Predictions: ')\n",
    "    print(pd.Series(y_pred).value_counts())\n",
    "    print(confusion_matrix(y, y_pred))\n",
    "    print(classification_report(y, y_pred))\n",
    "    print(f'ROC AUC: {roc_auc_score(y, y_pred):.4f}')\n",
    "    print(f'F1 Score: {f1_score(y, y_pred):.4f}')\n",
    "    print(f'Threshold: {threshold:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training on complete dataset')\n",
    "print(f'Dataset shape: {complete_dataset.shape}')\n",
    "print(f'Benign samples: {complete_dataset[1].value_counts()[0]}')\n",
    "print(f'Anomalies in dataset: {complete_dataset[1].value_counts().get(1, 0)}')\n",
    "print(f'Contamination rate: {complete_dataset[1].value_counts().get(1, 0) / complete_dataset[1].count() * 100:.4f} %\\n')\n",
    "\n",
    "# Train Isolation Forest\n",
    "clf_complete = IsolationForest(sample_size, n_trees)\n",
    "start_time = time.time()\n",
    "clf_complete.fit(complete_dataset[0], improved=False)\n",
    "end_time = time.time()\n",
    "print(f'{sample_size=}, {n_trees=}, training time: {end_time - start_time:.3f}s\\n')\n",
    "\n",
    "print('Predicting on complete dataset')\n",
    "start_time = time.time()\n",
    "scores = clf_complete.anomaly_score(complete_dataset[0])\n",
    "threshold, FPR = find_TPR_threshold(complete_dataset[1], scores, 0.9)\n",
    "y_pred = clf_complete.predict_from_anomaly_scores(scores, threshold)\n",
    "end_time = time.time()\n",
    "\n",
    "# Metrics\n",
    "plt.hist(scores, bins=30, label='Complete dataset')\n",
    "print(f'Prediction time: {end_time - start_time:.3f}s')\n",
    "print('Predictions: ')\n",
    "print(pd.Series(y_pred).value_counts())\n",
    "print(confusion_matrix(complete_dataset[1], y_pred))\n",
    "print(classification_report(complete_dataset[1], y_pred))\n",
    "print(f'ROC AUC: {roc_auc_score(complete_dataset[1], y_pred):.4f}')\n",
    "print(f'F1 Score: {f1_score(complete_dataset[1], y_pred):.4f}')\n",
    "print(f'Threshold: {threshold:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nIsolationForest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_203759/928402702.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msk_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskIsolationForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontamination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msk_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Sklearn training time: {end_time - start_time}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1472\u001b[0m                 )\n\u001b[1;32m   1473\u001b[0m             ):\n\u001b[0;32m-> 1474\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_iforest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \"\"\"\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;31m# Pre-sort indices to avoid that each individual tree of the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1050\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             )\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nIsolationForest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Compare with sklearn\n",
    "sk_clf = skIsolationForest(contamination=0.2, n_estimators=100, max_samples=256, n_jobs=-1, random_state=42)\n",
    "start_time = time.time()\n",
    "sk_clf.fit(X)\n",
    "end_time = time.time()\n",
    "print(f'Sklearn training time: {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "sk_scores = sk_clf.decision_function(X).reshape(-1, 1)\n",
    "sk_threshold, sk_FPR = find_TPR_threshold(y, sk_scores, 0.8)\n",
    "sk_y_pred = [1 if score >= sk_threshold else 0 for score in sk_scores]\n",
    "\n",
    "print(f'Confusion matrix: {confusion_matrix(y, sk_y_pred)}')\n",
    "print(f'Classification report: {classification_report(y, sk_y_pred)}')\n",
    "print(f'ROC AUC score: {roc_auc_score(y, sk_y_pred)}')\n",
    "print(f'Threshold: {sk_threshold}')\n",
    "print(f'FPR: {sk_FPR}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
